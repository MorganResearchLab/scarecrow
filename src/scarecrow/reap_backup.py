# -*- coding: utf-8 -*-
"""
#!/usr/bin/env python3
@author: David Wragg
"""

import ast
import gzip as gz
import logging
import os
import gc
import pandas as pd
import pysam
import re
import shutil
import multiprocessing as mp
from argparse import RawTextHelpFormatter
from collections import defaultdict
from functools import lru_cache
from typing import List, Tuple, Optional, Dict, Set
from scarecrow.logger import log_errors, setup_logger
from scarecrow.seed import parse_seed_arguments
from scarecrow.tools import generate_random_string


class BarcodeMatcherOptimized:
    def __init__(self, barcode_sequences: Dict[str, Set[str]], mismatches: int):
        self.mismatches = mismatches
        self.matchers = {}
        
        # Create optimized lookup structures for each whitelist
        for whitelist, sequences in barcode_sequences.items():
            exact_matches = set(sequences)
            # Create lookup tables for 1-mismatch sequences if needed
            mismatch_lookup = self._create_mismatch_lookup(sequences) if mismatches > 0 else None
            self.matchers[whitelist] = {
                'exact': exact_matches,
                'mismatch': mismatch_lookup,
                'length': len(next(iter(sequences)))
            }

    def _create_mismatch_lookup(self, sequences: Set[str]) -> Dict[str, str]:
        """Create a lookup table for sequences with 1 mismatch"""
        lookup = {}
        for seq in sequences:
            # Store the original sequence
            lookup[seq] = seq
            # Generate all 1-mismatch variants
            for i in range(len(seq)):
                for base in 'ACGTN':
                    if base != seq[i]:
                        variant = seq[:i] + base + seq[i+1:]
                        # Only store if this variant hasn't been seen or is closer to current sequence
                        if variant not in lookup:
                            lookup[variant] = seq
        return lookup

    @lru_cache(maxsize=1024)
    def _reverse_complement(self, sequence: str) -> str:
        """Cached reverse complement computation"""
        return str(Seq(sequence).reverse_complement())

    def find_match(self, sequence: str, whitelist: str, orientation: str) -> str:
        """Find best matching barcode sequence"""
        matcher = self.matchers[whitelist]
        barcode_len = matcher['length']
        
        if len(sequence) < barcode_len:
            return 'null'

        # Handle reverse orientation
        if orientation == 'reverse':
            sequence = self._reverse_complement(sequence)

        # Try exact match first
        if sequence in matcher['exact']:
            return sequence

        # If mismatches allowed, check mismatch lookup
        if self.mismatches > 0 and matcher['mismatch'] is not None:
            if sequence in matcher['mismatch']:
                return matcher['mismatch'][sequence]

        return 'null'
    

def parser_reap(parser):
    subparser = parser.add_parser(
        "reap",
        description="""
Extract sequence range from one fastq file of a pair, and annotate the sequence header with barcode 
sequences based on predicted positions generated by scarecrow harvest.

Example:

scarecrow reap --fastqs R1.fastq.gz R2.fastq.gz\n\t--barcode_positions barcode_positions.csv\n\t--barcodes BC1:v1_whitelist:bc1_whitelist.txt BC2:v2_whitelist:bc2_whitelist.txt BC3:v1_whitelist:bc3_whitelist.txt\n\t--read1 0-64 --out cdna.fastq.gz
---
""",
        epilog="The --barcodes <name> must match the barcode_whitelist values in the --barcode_positions file.",
        help="Extract sequence range from fastq files",
        formatter_class=RawTextHelpFormatter,
    )
    subparser.add_argument(
        "--fastqs", 
        nargs="+", 
        help="Pair of FASTQ files")
    subparser.add_argument(
        "-o", "--out",
        metavar="out.fastq.gz",
        help=("Path to output fastq file"),
        type=str,
        default='extracted.fastq',
    )
    subparser.add_argument(
        "-p", "--barcode_positions",
        metavar="barcode_positions",
        help=("File containing barcode positions, output by scarecrow harvest"),
        type=str,
        default=[],
    )
    subparser.add_argument(
        "-r", "--barcode_reverse_order",
        action='store_true',
        help='Reverse retrieval order of barcodes in barcode positions file [false]'
    )
    subparser.add_argument(
        "-j", "--jitter",
        metavar="jitter",
        type=int,
        default=5,
        help='Barcode position jitter [5]',
    )
    subparser.add_argument(
        "-m", "--mismatches",
        metavar="mismatches",
        type=int,
        default=1,
        help='Number of allowed mismatches in barcode [1]',
    )
    subparser.add_argument(
        "-x", "--extract",
        metavar="umi range",
        help=("Sequence range to extract <read>:<range> (e.g. 1:1-64)"),
        type=str,
        default=None
    )
    subparser.add_argument(
        "-u", "--umi",
        metavar="umi range",
        help=("Sequence range to extract for UMI <read>:<range> (e.g. 2:1-10)"),
        type=str,
        default=None
    )
    subparser.add_argument(
        "-c", "--barcodes",
        metavar="barcodes",
        nargs='+', 
        help='Barcode whitelist files in format <name>:<file> (e.g. BC1:barcodes1.txt BC2:barcodes2.txt)',
    )
    subparser.add_argument(
        "-b", "--batch_size",
        metavar="batch_size",
        help=("Number of read pairs per batch to process at a time [10000]"),
        type=int,
        default=10000,
    )
    subparser.add_argument(
        "-@", "--threads",
        metavar="threads",
        help=("Number of processing threads [1]"),
        type=int,
        default=1,
    )
    subparser.add_argument(
        "-v", "--verbose",
        action='store_true',
        help='Enable verbose output [false]'
    )
    subparser.add_argument(
        "-z", "--gzip",
        action='store_true',
        help='Compress (gzip) fastq output [false]'
    )
    return subparser

def validate_reap_args(parser, args) -> None:
    """ 
    Validate arguments 
    """
    run_reap(fastqs = [f for f in args.fastqs], 
             barcode_positions = args.barcode_positions,
             barcode_reverse_order = args.barcode_reverse_order,
             output = args.out,
             extract = args.extract,
             umi = args.umi, 
             barcodes = args.barcodes,
             jitter = args.jitter,
             mismatches = args.mismatches,
             batches = args.batch_size, 
             threads = args.threads,
             verbose = args.verbose,
             gzip = args.gzip)

@log_errors
def run_reap(fastqs: List[str], 
             barcode_positions: str = None,
             barcode_reverse_order: bool = False,
             output: str = 'extracted.fastq',
             extract: str = None,
             umi: Optional[str] = None,
             barcodes: List[str] = None,
             jitter: int = 5,
             mismatches: int = 1,
             batches: int = 10000,
             threads: int = 4,
             verbose: bool = False,
             gzip: bool = False) -> None:
    """
    Main function to extract sequences with barcode headers
    """    
    # Global logger setup
    logfile = '{}_{}.{}'.format('./scarecrow_reap', generate_random_string(), 'log')
    logger = setup_logger(logfile)
    logger.info(f"logfile: '{logfile}'")

    # Extract barcodes and convert whitelist to set
    expected_barcodes = parse_seed_arguments(barcodes)  
    for key, barcode in expected_barcodes.items():
        expected_barcodes[key] = sorted(set(barcode))
        if verbose:
            logger.info(f"{key}: {barcode}")

    # Check if the output filename string ends with .gz
    if output.endswith(".gz"):
        output = output[:-3]

    # Extract sequences
    extract_sequences(
        fastq_files = [f for f in fastqs],
        barcode_positions_file = barcode_positions,
        barcode_reverse_order = barcode_reverse_order,
        barcode_sequences = expected_barcodes,
        output = output,
        extract = extract,
        umi = umi,
        jitter = jitter,
        mismatches = mismatches,
        batch_size = batches,
        threads = threads,
        verbose = verbose
    )
    
    # gzip
    if gzip:
        logger.info(f"Compressing '{output}'")
        with open(output, 'rb') as f_in, gz.open(output + ".gz", 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)
        os.remove(output)

@log_errors
def process_read_batch(read_batch: List[Tuple], 
                      barcode_configs: List[Dict],
                      matcher: BarcodeMatcherOptimized,
                      read_range: Tuple[int, int],
                      read_index: int,
                      umi_index: int,
                      umi_range: Tuple[int, int],
                      verbose: bool) -> List[str]:
    """
    Process a batch of reads with optimized matching
    """
    logger = logging.getLogger('scarecrow')

    output_entries = []
    
    for reads in read_batch:
        barcodes = []
        for config in barcode_configs:
            seq = reads[config['file_index']].sequence
            start, end = config['start'], config['end']
            barcode_seq = seq[start-1:end]

            if verbose:
                logger.info(f"Read: {reads[config['file_index']].name} {reads[config['file_index']].comment}")
                logger.info(f"Sequence: {reads[config['file_index']].sequence}")

            whitelist = ast.literal_eval(config['whitelist'])

            # This condition check was added due to whitelist being read as a list when passed via SLURM
            if isinstance(whitelist, list) and len(whitelist) == 1 and isinstance(whitelist[0], tuple):
                whitelist = whitelist[0]

            if verbose:
                logger.info(f"whitelist: {tuple(whitelist)}")
                logger.info(f"matcher.matchers: {matcher.matchers}")

            if whitelist in matcher.matchers:
                matched_barcode = matcher.find_match(
                    barcode_seq, whitelist, config['orientation'])
                if verbose:
                    logger.info(f"Matched barcode: {matched_barcode} for {barcode_seq} @ range {start}-{end}")

                barcodes.append(matched_barcode)
            else:
                barcodes.append('null')

        # Extract sequence and create output
        source_entry = reads[read_index]
        extract_seq = source_entry.sequence[read_range[0]:read_range[1]]
        extract_qual = source_entry.quality[read_range[0]:read_range[1]] 
        
        # FASTQ sequence header
        header = f"@{source_entry.name} {source_entry.comment} barcodes={('_').join(barcodes)}"

        # Add UMI to header if requested
        if umi_index is not None:
            umi_seq = reads[umi_index].sequence[umi_range[0]:umi_range[1]]
            header = f"@{header} UMI={umi_seq}"

        output_entries.append(f"{header}\n{extract_seq}\n+\n{extract_qual}\n")

    return output_entries

@log_errors
def extract_sequences(
    fastq_files: List[str] = None,
    barcode_positions_file: str = None,
    barcode_reverse_order: bool = False,
    barcode_sequences: Dict[str, List[str]] = None,
    output: str = 'extracted.fastq.gz',
    extract: str = None,
    umi: Optional[str] = None,
    jitter: int = 5,
    mismatches: int = 1,
    batch_size: int = 100000,
    threads: Optional[int] = None,
    verbose: bool = False
) -> None:
    """
    Optimized sequence extraction focused on matching performance
    """
    logger = logging.getLogger('scarecrow')

    # Initialize configurations
    barcode_positions = pd.read_csv(barcode_positions_file)
    if barcode_reverse_order:
        barcode_positions = barcode_positions[::-1].reset_index(drop=True)
    barcode_configs = prepare_barcode_configs(barcode_positions, jitter)

    # Extract range
    extract_index, extract_range = extract.split(':')
    extract_index = int(extract_index)-1
    extract_range = parse_range(extract_range)
    logger.info(f"FASTQ sequence range to extract: '{extract}'")

    # UMI range
    if umi is not None:
        umi_index, umi_range = umi.split(':')
        umi_index = int(umi_index)-1
        umi_range = parse_range(umi_range)
        logger.info(f"UMI sequence range to extract: '{umi}'")
    else:
        umi_index = None
        umi_range = None

    # Create optimized matcher
    logger.info(f"Generating barcode matcher")
    matcher = BarcodeMatcherOptimized(
        barcode_sequences = {k: set(v) for k, v in barcode_sequences.items()},
        mismatches = mismatches
    )
       
    # Process files with minimal overhead
    if threads is None:
        threads = min(mp.cpu_count() - 1, 8)
    else:
        threads = min(threads, mp.cpu_count())
    logger.info(f"Using {threads} threads")

    # List of files generated
    files = []

    def write_and_clear_results(jobs):
        """Retrieve results from completed jobs, write to file, and free memory."""
        for idx, job in enumerate(jobs):
            results = job.get()
            outfile = output + "_" + str(idx)
            if outfile not in files:
                files.append(outfile)
                if os.path.exists(outfile):
                    os.remove(outfile)
            with open(outfile, 'a') as out_fastq:
                out_fastq.writelines(results)
            del results                
        jobs.clear()
        gc.collect()
    
    def combine_results_chunked(files, output, chunk_size=1024*1024):
        with open(output, 'w') as out_fastq:
            for fastq_file in files:
                with open(fastq_file, 'r') as in_fastq:
                    while chunk := in_fastq.read(chunk_size):
                        out_fastq.write(chunk)
                os.remove(fastq_file)

    logger.info(f"Processing reads")
    with pysam.FastqFile(fastq_files[0]) as r1, \
         pysam.FastqFile(fastq_files[1]) as r2:
        
        # Create batches efficiently
        read_pairs = zip(r1, r2)        

        pool = mp.Pool(threads)
        batch = []
        jobs = []
        counter = 0       

        for reads in read_pairs:            
            batch.append(reads)            
            if len(batch) >= batch_size:
                jobs.append(pool.apply_async(worker_task, args=((batch, barcode_configs, matcher, 
                                                                 extract_range, extract_index, 
                                                                 umi_index, umi_range, verbose),)))
                counter += len(batch)                

                # Write results and free memory if jobs exceed a threshold
                if len(jobs) >= threads:
                    write_and_clear_results(jobs)
                    logger.info(f"Processed {counter} reads")
                
                # Clear the current batch
                batch = []                
        
        # Process remaining reads
        if batch:
            jobs.append(pool.apply_async(worker_task, args=((batch, barcode_configs, matcher, 
                                                                 extract_range, extract_index, 
                                                                 umi_index, umi_range, verbose),)))
            
        write_and_clear_results(jobs)
        logger.info(f"Processed {counter} reads")
        
        # Close pools
        pool.close()
        pool.join()

    # Combine results
    logger.info(f"Combining results: {files}")
    combine_results_chunked(files, output)


def parse_range(range_str: str) -> Tuple[int, int]:
    """
    Parse range string
    """
    start, end = map(int, range_str.split('-'))
    start = max(0, start -1)
    return (start, end)

def prepare_barcode_configs(positions: pd.DataFrame, jitter: int) -> List[Dict]:
    """
    Prepare barcode configurations
    """
    return [{
        'index': idx,
        'file_index': 0 if row['read'] == 'read1' else 1,
        'start': row['start'],
        'end': row['end'],
        'orientation': row['orientation'],
        'whitelist': row['barcode_whitelist']
    } for idx, row in positions.iterrows()]

def worker_task(args):
    return process_read_batch(*args)
