#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@author: David Wragg
"""

from argparse import RawTextHelpFormatter
from scarecrow.fastq_logging import log_errors, setup_logger, logger
import numpy as np
import pandas as pd
from scipy.signal import find_peaks

def parser_harvest(parser):
    subparser = parser.add_parser(
        "harvest",
        description="""
Harvest barcode start-end positions from barcode alignment distributions.

Example:

scarecrow harveset BC1.csv BC2.csv BC3.csv --barcode_count 3 --min_distance 10
---
""",
        help="Harvest barcode start-end positions",
        formatter_class=RawTextHelpFormatter,
    )
    subparser.add_argument("barcodes", nargs="+", help="List of scarecrow barcode CSV output files")
    subparser.add_argument(
        "-o", "--out",
        metavar="barcodes_positions",
        help=("Path to output barcode positions file"),
        type=str,
        default="./barcode_positions.csv",
    )
    subparser.add_argument(
        "-b", "--barcode_count",
        metavar="barcode_count",
        help=("Number of barcodes expected [3]"),
        type=int,
        default=3,
    )    
    subparser.add_argument(
        "-d", "--min_distance",
        metavar="min_distance",
        help=("Minimum distance between peaks [10]"),
        type=int,
        default=10,
    )
    subparser.add_argument(
        "-l", "--logfile",
        metavar="logfile",
        help=("File to write log to"),
        type=str,
        default="./scarecrow.log",
    )
    return subparser

def validate_harvest_args(parser, args):
    run_harvest(barcodes = args.barcodes,
                output_file = args.out,
                num_barcodes = args.barcode_count,
                min_distance = args.min_distance,
                logfile = args.logfile)

def run_harvest(barcodes, output_file, num_barcodes, min_distance, logfile):
    """
    Extract barcode positions from distribution of barcode alignments 
    Expects as an input a series of CSV files generated by scarecrow barcodes
    """
    
    # Global logger setup
    logger = setup_logger(logfile)

    # Read barcode CSV files into a dataframe using pandas
    results = get_barcode_peaks(barcodes)
    top_peaks = top_peak_positions(results, num_barcodes, min_distance)
    print("\033[34m\nTop peaks\033[0m")
    print(f"{top_peaks}\n")

    # Open file for writing output
    if output_file:
        top_peaks.to_csv(output_file, index = False)

    return 



def find_peaks_with_details(start_positions, end_positions, names):
    """
    Find peaks with detailed information about unique reads and names
    
    Args:
    - start_positions: Array of start positions
    - end_positions: Array of corresponding end positions
    - names: Array of corresponding read names
    
    Returns:
    - List of peak details: (start_position, end_position, unique_read_count, unique_name_fraction)
    """
    # Count frequency of each start position
    position_counts = pd.Series(start_positions).value_counts().sort_index()
    
    # Identify peaks in the frequency data
    scipy_peaks, _ = find_peaks(position_counts.values)
    
    # Extract peak positions with detailed information
    peaks_with_details = []
    for p in scipy_peaks:
        start = position_counts.index[p]
 
        # Filter to positions matching this start position
        mask = start_positions == start
        peak_names = names[mask]
        end = end_positions[mask][0]  # Get the corresponding end position
                
        # Calculate unique read count and name fraction
        peak_unique_names = np.unique(peak_names)
        read_count = len(peak_unique_names)
        
        # Ensure we don't divide by zero
        unique_names_count = len(np.unique(names))
        read_fraction = round(read_count / max(unique_names_count, 1),2)
              
        peaks_with_details.append((start, end, read_count, read_fraction))
    
    # Sort peaks by read_count in descending order
    peaks_with_details.sort(key=lambda x: x[2], reverse=True)
    
    return peaks_with_details



def get_barcode_peaks(barcodes: list):
    """
    Read barcode CSV data in and identify peaks
    
    Args:
    - barcodes: List of CSV file paths
    """
    # Read and concatenate all barcode data
    barcode_data = pd.concat([pd.read_csv(file, sep='\t') for file in barcodes], ignore_index=True)

    # Identify barcode alignment peaks
    results = []
    barcode_groups = barcode_data.groupby(["read", "barcode_whitelist", "orientation"])
    for (read, barcode_whitelist, orientation), group in barcode_groups:
        # Find peaks with detailed information
        peaks = find_peaks_with_details(
            group["start"].values, 
            group["end"].values, 
            group["name"].values
        )
        
        # Add some debug logging
        logger.info(f"Peaks for {read} {barcode_whitelist} {orientation}:")
        for peak in peaks:
            logger.info(f"  Peak: start={peak[0]}, end={peak[1]}, read_count={peak[2]}, read_fraction={peak[3]}")
        
        # Store the results
        results.append({
            "read": read,
            "barcode_whitelist": barcode_whitelist,
            "orientation": orientation,
            "peaks": [
                {
                    "start": peak[0], 
                    "end": peak[1], 
                    "read_count": peak[2],
                    "read_fraction": peak[3]
                } for peak in peaks
            ]
        })

    # Sort the results by the highest unique read count
    sorted_results = sorted(
        results,
        key=lambda x: max([peak["read_count"] for peak in x["peaks"]] or [0]),
        reverse=True
    )

    return sorted_results


def top_peak_positions(results, num_barcodes, min_distance):
    """
    Select top peaks, ensuring non-overlapping and minimum distance
    
    Args:
    - results: List of barcode peak results
    - num_barcodes: Number of barcodes to select
    - min_distance: Minimum distance between peaks
    """
    # Flatten the results to include individual peaks
    flattened_results = []
    for result in results:
        for peak in result["peaks"]:
            flattened_results.append({
                "start": peak["start"],
                "end": peak["end"],
                "read_count": peak["read_count"],
                "read_fraction": peak["read_fraction"],
                "barcode_whitelist": result["barcode_whitelist"],
                "read": result["read"],
                "orientation": result["orientation"]
            })

    # Convert to a DataFrame
    flattened_df = pd.DataFrame(flattened_results)

    # Group by position and barcode_whitelist, then aggregate
    grouped = flattened_df.groupby(
        ["read", "start", "end", "orientation", "barcode_whitelist"], 
        as_index=False
    ).agg({
        "read_count": "sum",
        "read_fraction": "mean"
    })

    # Sort by unique read count (descending) and position (ascending)
    sorted_grouped = grouped.sort_values(by=["read_count", "start"], ascending=[False, True])

    # Select top peaks with non-overlapping and minimum distance constraints
    top_positions = []
    for _, row in sorted_grouped.iterrows():
        # Check if this peak is far enough from all previously selected peaks
        if not any(
            abs(row["start"] - selected_peak["end"]) < min_distance or (row["start"] == selected_peak["start"])
            for selected_peak in top_positions
        ):
            top_positions.append(row)
            
            # Stop if we've found the desired number of peaks
            if len(top_positions) == num_barcodes:
                break

    # Create a DataFrame of the results
    top_positions_df = pd.DataFrame(top_positions).sort_values(by="start", ascending=True)

    return top_positions_df