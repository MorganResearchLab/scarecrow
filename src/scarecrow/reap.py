# -*- coding: utf-8 -*-
"""
#!/usr/bin/env python3
@author: David Wragg
"""

import ast
import gzip as gz
import logging
import os
import gc
import pandas as pd
import numpy as np
import pysam
import re
import shutil
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from argparse import RawTextHelpFormatter
from collections import defaultdict
from functools import lru_cache, partial
from typing import List, Tuple, Optional, Dict, Set
from io import StringIO
from itertools import islice
from scarecrow.logger import log_errors, setup_logger
from scarecrow.seed import parse_seed_arguments
from scarecrow.tools import generate_random_string



def parser_reap(parser):
    subparser = parser.add_parser(
        "reap",
        description="""
Extract sequence range from one fastq file of a pair, and annotate the sequence header with barcode 
sequences based on predicted positions generated by scarecrow harvest.

Example:

scarecrow reap --fastqs R1.fastq.gz R2.fastq.gz\n\t--barcode_positions barcode_positions.csv\n\t--barcodes BC1:v1_whitelist:bc1_whitelist.txt BC2:v2_whitelist:bc2_whitelist.txt BC3:v1_whitelist:bc3_whitelist.txt\n\t--read1 0-64 --out cdna.fastq.gz
---
""",
        epilog="The --barcodes <name> must match the barcode_whitelist values in the --barcode_positions file.",
        help="Extract sequence range from fastq files",
        formatter_class=RawTextHelpFormatter,
    )
    subparser.add_argument(
        "--fastqs", 
        nargs="+", 
        help="Pair of FASTQ files")
    subparser.add_argument(
        "-o", "--out",
        metavar="out.fastq.gz",
        help=("Path to output fastq file"),
        type=str,
        default='extracted.fastq',
    )
    subparser.add_argument(
        "-p", "--barcode_positions",
        metavar="barcode_positions",
        help=("File containing barcode positions, output by scarecrow harvest"),
        type=str,
        default=[],
    )
    subparser.add_argument(
        "-r", "--barcode_reverse_order",
        action='store_true',
        help='Reverse retrieval order of barcodes in barcode positions file [false]'
    )
    subparser.add_argument(
        "-j", "--jitter",
        metavar="jitter",
        type=int,
        default=5,
        help='Barcode position jitter [5]',
    )
    subparser.add_argument(
        "-m", "--mismatches",
        metavar="mismatches",
        type=int,
        default=1,
        help='Number of allowed mismatches in barcode [1]',
    )
    subparser.add_argument(
        "-x", "--extract",
        metavar="umi range",
        help=("Sequence range to extract <read>:<range> (e.g. 1:1-64)"),
        type=str,
        default=None
    )
    subparser.add_argument(
        "-u", "--umi",
        metavar="umi range",
        help=("Sequence range to extract for UMI <read>:<range> (e.g. 2:1-10)"),
        type=str,
        default=None
    )
    subparser.add_argument(
        "-c", "--barcodes",
        metavar="barcodes",
        nargs='+', 
        help='Barcode whitelist files in format <name>:<file> (e.g. BC1:barcodes1.txt BC2:barcodes2.txt)',
    )
    subparser.add_argument(
        "-b", "--batch_size",
        metavar="batch_size",
        help=("Number of read pairs per batch to process at a time [10000]"),
        type=int,
        default=10000,
    )
    subparser.add_argument(
        "-@", "--threads",
        metavar="threads",
        help=("Number of processing threads [1]"),
        type=int,
        default=1,
    )
    subparser.add_argument(
        "-v", "--verbose",
        action='store_true',
        help='Enable verbose output [false]'
    )
    subparser.add_argument(
        "-z", "--gzip",
        action='store_true',
        help='Compress (gzip) fastq output [false]'
    )
    return subparser

def validate_reap_args(parser, args) -> None:
    """ 
    Validate arguments 
    """
    run_reap(fastqs = [f for f in args.fastqs], 
             barcode_positions = args.barcode_positions,
             barcode_reverse_order = args.barcode_reverse_order,
             output = args.out,
             extract = args.extract,
             umi = args.umi, 
             barcodes = args.barcodes,
             jitter = args.jitter,
             mismatches = args.mismatches,
             batches = args.batch_size, 
             threads = args.threads,
             verbose = args.verbose,
             gzip = args.gzip)

@log_errors
def run_reap(fastqs: List[str], 
             barcode_positions: str = None,
             barcode_reverse_order: bool = False,
             output: str = 'extracted.fastq',
             extract: str = None,
             umi: Optional[str] = None,
             barcodes: List[str] = None,
             jitter: int = 5,
             mismatches: int = 1,
             batches: int = 10000,
             threads: int = 4,
             verbose: bool = False,
             gzip: bool = False) -> None:
    """
    Optimized main function to extract sequences with barcode headers
    """    
    # Use process-local logging to avoid I/O contention
    logfile = '{}_{}.{}'.format('./scarecrow_reap', generate_random_string(), 'log')
    logger = setup_logger(logfile)
    logger.info(f"logfile: '{logfile}'")

    # Pre-compile regex patterns
    BARCODE_PATTERN = re.compile(r'barcodes=([\w_]+)')
    
    # Load and preprocess barcodes into character arrays
    expected_barcodes = {}
    for key, barcode_list in parse_seed_arguments(barcodes).items():
        # Convert each barcode into a character array
        barcode_arrays = [np.array(list(barcode)) for barcode in sorted(set(barcode_list))]
        if barcode_arrays:
            # Stack arrays vertically to create a 2D array
            expected_barcodes[key] = np.vstack(barcode_arrays)
    
    if verbose:
        for key, barcode in expected_barcodes.items():
            logger.info(f"{key}: {barcode.shape}")

    # Strip .gz extension if present
    output = output[:-3] if output.endswith(".gz") else output

    # Extract sequences using process pool for CPU-bound tasks
    with ProcessPoolExecutor(max_workers=threads) as executor:
        extract_sequences_parallel(
            executor=executor,
            fastq_files=fastqs,
            barcode_positions_file=barcode_positions,
            barcode_reverse_order=barcode_reverse_order,
            barcode_sequences=expected_barcodes,
            output=output,
            extract=extract,
            umi=umi,
            jitter=jitter,
            mismatches=mismatches,
            batch_size=batches,
            verbose=verbose
        )

    # Process headers using memory-efficient streaming
    barcode_counts, cell_barcodes = process_fastq_headers_streaming(
        output, BARCODE_PATTERN, chunk_size=1024*1024
    )

    # Write results using buffered I/O
    write_results(output, barcode_counts, cell_barcodes, verbose)

    # Compress if needed using larger buffer size
    if gzip:
        logger.info(f"Compressing '{output}'")
        with open(output, 'rb') as f_in:
            with gz.open(output + ".gz", 'wb', compresslevel=1) as f_out:
                shutil.copyfileobj(f_in, f_out, length=1024*1024)
        os.remove(output)

def process_fastq_headers(file_path: str = None) -> Tuple[List[defaultdict[str, int]], defaultdict[str, int]]:
    """
    Process fastq header to report on barcode counts

    Args:
        file_path: fastq file to operate on
    
    Returns:
        (1) list of dictionary of barcode counts, and (2) dictionary of barcode combination counts
    """

    # Create a list of dictionaries, one for each barcode position
    barcode_counts = []
    cell_barcodes = defaultdict(int)

    # Open the FASTQ file using pysam
    with pysam.FastxFile(file_path) as fastq_file:
        for entry in fastq_file:
            # Extract the header line
            header = entry.comment

            # Check if the header contains 'barcodes='
            if 'barcodes=' in header:
                # Extract the barcodes substring
                barcodes_str = re.search(r'barcodes=([\w_]+)', header).group(1)
                cell_barcodes[barcodes_str] += 1
                
                # Split the barcodes string by underscore
                barcodes = barcodes_str.split('_')
                
                # Ensure barcode_counts has enough dictionaries for all barcode positions
                while len(barcode_counts) < len(barcodes):
                    barcode_counts.append(defaultdict(int))
                
                # Update counts in the corresponding dictionaries
                for i, barcode in enumerate(barcodes):
                    barcode_counts[i][barcode] += 1

    return barcode_counts, cell_barcodes


class OptimizedBarcodeMatcher:
    """Memory-efficient barcode matcher using numpy arrays"""
    def __init__(self, barcode_sequences: Dict[str, np.ndarray], mismatches: int):
        self.mismatches = mismatches
        self.matchers = {}
        
        for whitelist, sequences in barcode_sequences.items():
            self.matchers[whitelist] = {
                'sequences': sequences,  # Already a 2D array from preprocessing
                'length': sequences.shape[1] if sequences.size > 0 else 0
            }

    @lru_cache(maxsize=10000)
    def find_match(self, sequence: str, whitelist: str, orientation: str) -> str:
        """Vectorized barcode matching"""
        matcher = self.matchers.get(whitelist)
        if not matcher or len(sequence) != matcher['length']:
            return 'null'

        # Convert sequence to 2D array for broadcasting
        seq_array = np.array(list(sequence))[np.newaxis, :]
        
        # Vectorized hamming distance calculation with proper broadcasting
        # sequences shape is (n_sequences, sequence_length)
        # seq_array shape is (1, sequence_length)
        distances = np.sum(seq_array != matcher['sequences'], axis=1)
        min_dist_idx = np.argmin(distances)
        
        if distances[min_dist_idx] <= self.mismatches:
            return ''.join(matcher['sequences'][min_dist_idx])
        return 'null'

@log_errors
def process_read_batch_vectorized(read_batch: List[Tuple],
                                barcode_configs: List[Dict],
                                matcher: OptimizedBarcodeMatcher,
                                read_range: Tuple[int, int],
                                read_index: int,
                                umi_index: int,
                                umi_range: Tuple[int, int]) -> List[str]:
    """Vectorized batch processing using numpy"""
    output_buffer = StringIO()
    
    for reads in read_batch:
        barcodes = []
        for config in barcode_configs:
            seq = reads[config['file_index']].sequence
            start, end = config['start'] - 1, config['end']
            barcode_seq = seq[start:end]
            
            whitelist = ast.literal_eval(config['whitelist'])
            if isinstance(whitelist, list) and len(whitelist) == 1:
                whitelist = whitelist[0]
                
            matched_barcode = matcher.find_match(
                barcode_seq, whitelist, config['orientation'])
            barcodes.append(matched_barcode)

        # Extract sequences
        source_entry = reads[read_index]
        extract_seq = source_entry.sequence[read_range[0]:read_range[1]]
        extract_qual = source_entry.quality[read_range[0]:read_range[1]]
        
        umi_seq = reads[umi_index].sequence[umi_range[0]:umi_range[1]]
        
        # Write to buffer
        output_buffer.write(
            f"@{source_entry.name} {source_entry.comment} "
            f"barcodes={('_').join(barcodes)} UMI={umi_seq}\n"
            f"{extract_seq}\n+\n{extract_qual}\n"
        )
    
    return output_buffer.getvalue()

@log_errors
def extract_sequences_parallel(executor: ProcessPoolExecutor,
                             fastq_files: List[str],
                             barcode_positions_file: str,
                             barcode_reverse_order: bool,
                             barcode_sequences: Dict[str, np.ndarray],
                             output: str,
                             extract: str,
                             umi: str,
                             jitter: int,
                             mismatches: int,
                             batch_size: int,
                             verbose: bool) -> None:
    """Parallel sequence extraction using process pool"""
    # Initialize configurations
    barcode_positions = pd.read_csv(barcode_positions_file)
    if barcode_reverse_order:
        barcode_positions = barcode_positions[::-1].reset_index(drop=True)
    
    barcode_configs = [
        {
            'index': idx,
            'file_index': 0 if row['read'] == 'read1' else 1,
            'start': row['start'],
            'end': row['end'],
            'orientation': row['orientation'],
            'whitelist': row['barcode_whitelist']
        }
        for idx, row in barcode_positions.iterrows()
    ]

    # Parse ranges
    extract_index, extract_range = extract.split(':')
    extract_index = int(extract_index) - 1
    start, end = map(int, extract_range.split('-'))
    extract_range = (max(0, start - 1), end)

    umi_index, umi_range = umi.split(':')
    umi_index = int(umi_index) - 1
    start, end = map(int, umi_range.split('-'))
    umi_range = (max(0, start - 1), end)

    # Initialize matcher
    matcher = OptimizedBarcodeMatcher(barcode_sequences, mismatches)

    # Process in parallel
    with pysam.FastqFile(fastq_files[0]) as r1, \
         pysam.FastqFile(fastq_files[1]) as r2:
        
        def batch_generator():
            while True:
                batch = list(islice(zip(r1, r2), batch_size))
                if not batch:
                    break
                yield batch

        process_batch = partial(
            process_read_batch_vectorized,
            barcode_configs=barcode_configs,
            matcher=matcher,
            read_range=extract_range,
            read_index=extract_index,
            umi_index=umi_index,
            umi_range=umi_range
        )

        with open(output, 'w') as out_file:
            for result in executor.map(process_batch, batch_generator()):
                out_file.write(result)

def process_fastq_headers_streaming(file_path: str,
                                  pattern: re.Pattern,
                                  chunk_size: int = 1024*1024) -> Tuple[List[defaultdict[str, int]], defaultdict[str, int]]:
    """Stream process fastq headers to reduce memory usage"""
    barcode_counts = []
    cell_barcodes = defaultdict(int)
    
    with open(file_path, 'r') as f:
        header = ''
        for chunk in iter(lambda: f.read(chunk_size), ''):
            lines = (header + chunk).split('\n')
            header = lines[-1]
            
            for line in lines[:-1]:
                if line.startswith('@'):
                    if match := pattern.search(line):
                        barcodes_str = match.group(1)
                        cell_barcodes[barcodes_str] += 1
                        
                        barcodes = barcodes_str.split('_')
                        while len(barcode_counts) < len(barcodes):
                            barcode_counts.append(defaultdict(int))
                        
                        for i, barcode in enumerate(barcodes):
                            barcode_counts[i][barcode] += 1
    
    return barcode_counts, cell_barcodes

def parse_range(range_str: str) -> Tuple[int, int]:
    """
    Parse range string
    """
    start, end = map(int, range_str.split('-'))
    start = max(0, start -1)
    return (start, end)

def prepare_barcode_configs(positions: pd.DataFrame, jitter: int) -> List[Dict]:
    """
    Prepare barcode configurations
    """
    return [{
        'index': idx,
        'file_index': 0 if row['read'] == 'read1' else 1,
        'start': row['start'],
        'end': row['end'],
        'orientation': row['orientation'],
        'whitelist': row['barcode_whitelist']
    } for idx, row in positions.iterrows()]

def worker_task(args):
    return process_read_batch(*args)

@log_errors
def write_results(output: str, 
                 barcode_counts: List[defaultdict[str, int]], 
                 cell_barcodes: defaultdict[str, int],
                 verbose: bool = False) -> None:
    """
    Write barcode counts and combinations to CSV files efficiently
    
    Args:
        output: Base output filename
        barcode_counts: List of dictionaries containing counts for each barcode position
        cell_barcodes: Dictionary of combined barcode counts
        verbose: Whether to log detailed information
    """
    logger = logging.getLogger('scarecrow')
    
    # Write individual barcode counts
    barcode_file = f"{output}.barcodes.csv"
    with open(barcode_file, 'w') as f:
        f.write("Index,Barcode,Count\n")
        for i, counts in enumerate(barcode_counts):
            if verbose:
                for barcode, count in counts.items():
                    logger.info(f"Barcode index: {i + 1}\tBarcode: {barcode}\tCount: {count}")
            
            # Sort by count for better visualization
            sorted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)
            for barcode, count in sorted_counts:
                f.write(f"{i + 1},{barcode},{count}\n")

    # Write combined barcode counts
    combinations_file = f"{output}.barcode.combinations.csv"
    with open(combinations_file, 'w') as f:
        f.write("BarcodeCombination,Count\n")
        if verbose:
            for cell, count in cell_barcodes.items():
                logger.info(f"Barcode combination: {cell}\tCount: {count}")
        
        # Sort by count for better visualization
        sorted_combinations = sorted(cell_barcodes.items(), key=lambda x: x[1], reverse=True)
        for cell, count in sorted_combinations:
            f.write(f"{cell},{count}\n")